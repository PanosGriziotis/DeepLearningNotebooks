{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ula9d7Jjjhgr",
   "metadata": {
    "id": "ula9d7Jjjhgr"
   },
   "source": [
    "# Word2Vec model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stdiecstqQv3",
   "metadata": {
    "id": "stdiecstqQv3"
   },
   "source": [
    "**Description**\n",
    "\n",
    "In this notebook the goal is to train and evaluate word2vec models with different hyperparameter tunings using the gensim library. The task is to spot diversities between the results of the models' evaluation and comment on the findings. The dataset used is a corpus of news articles in English originally extracted from http://www.setimes.com  originally consisted of 100.000 sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "QYtWOKTm1qYn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYtWOKTm1qYn",
    "outputId": "671a2ea5-faf3-4e3d-af46-9cb1f871ec44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: gensim in c:\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\panos\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "CtvOnr7rxDu0",
   "metadata": {
    "id": "CtvOnr7rxDu0"
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zekKTqd456R1",
   "metadata": {
    "id": "zekKTqd456R1"
   },
   "source": [
    "**1.** **Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac8ca1c",
   "metadata": {
    "id": "2ac8ca1c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def read_file (filepath):\n",
    "    with open (filepath, 'r', encoding='utf8', errors='ignore') as reader:\n",
    "        lines = [line for line in reader]\n",
    "        return lines\n",
    "\n",
    "def cleaning(lines):\n",
    "    cleaned_sentences = []\n",
    "    for text in lines: \n",
    "        text = re.sub (r'[^\\w\\s]', '', text)# remove special characters\n",
    "        text = re.sub(r'(\\w\\s)\\1{2,}', '', text) # remove multiple consequtive occurrences of a character\n",
    "        text = re.sub(r'[0-9]+', '', text) # remove digits\n",
    "        text = text.lower().rstrip() # set everything to lower case\n",
    "        words = [word for word in text.split(' ')]\n",
    "        if text != '' and len(words) > 2: # drop sentences with few words\n",
    "            cleaned_sentences.append(text)\n",
    "        else:\n",
    "            continue\n",
    "    return cleaned_sentences\n",
    "\n",
    "def tokenize (list_of_sentences):\n",
    "    tokenized_sent = []\n",
    "    for sent in list_of_sentences:\n",
    "        tokenized_sent.append(sent.split())\n",
    "    return tokenized_sent\n",
    "\n",
    "def drop_duplicates (cleaned_sentences):\n",
    "    df_clean = pd.DataFrame({'sentences': cleaned_sentences})\n",
    "    df_clean = df_clean.dropna().drop_duplicates()\n",
    "    return df_clean['sentences'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqWNrXhMvk5d",
   "metadata": {
    "id": "vqWNrXhMvk5d"
   },
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addca050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "addca050",
    "outputId": "d0d22332-e15b-4b44-e212-d37446102f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['croatia', 'farming', 'and', 'agriculture', 'sectors', 'could', 'reap', 'benefits', 'from', 'eu', 'membership'], ['on', 'friday', 'the', 'eu', 'confirmed', 'its', 'commitment', 'to', 'admitting', 'bulgaria', 'and', 'romania', 'together', 'in', 'although', 'the', 'two', 'countries', 'are', 'at', 'different', 'stages', 'of', 'their', 'accession', 'preparations']]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = read_file(\"setimes_english.txt\")\n",
    "# Apply pre-proccesing\n",
    "sentences = tokenize(drop_duplicates(cleaning(dataset)))\n",
    "print (sentences[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zOYhel226yXA",
   "metadata": {
    "id": "zOYhel226yXA"
   },
   "source": [
    "**2. Set parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "712c84fd",
   "metadata": {
    "id": "712c84fd"
   },
   "outputs": [],
   "source": [
    "# model 1 \n",
    "model_1 = gensim.models.Word2Vec (\n",
    "    vector_size=300,   \n",
    "    window=10,   \n",
    "    min_count=10, \n",
    "    workers=10,  \n",
    "    sg=0,        \n",
    "    hs=0,\n",
    "    negative=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "jZfcWgRR7cZv",
   "metadata": {
    "id": "jZfcWgRR7cZv"
   },
   "outputs": [],
   "source": [
    "# model 2\n",
    "model_2 = gensim.models.Word2Vec (\n",
    "    vector_size=300,   \n",
    "    window=10,   \n",
    "    min_count=10, \n",
    "    workers=10,  \n",
    "    sg=1,        \n",
    "    hs=0,\n",
    "    negative=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "q5moq_T07nFq",
   "metadata": {
    "id": "q5moq_T07nFq"
   },
   "outputs": [],
   "source": [
    "# model 3\n",
    "model_3 = gensim.models.Word2Vec (\n",
    "    vector_size = 100,   \n",
    "    window=2,   \n",
    "    min_count=2, \n",
    "    workers=10,  \n",
    "    sg=0,        \n",
    "    hs=0,\n",
    "    negative=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "zPRWm9wm7nQR",
   "metadata": {
    "id": "zPRWm9wm7nQR"
   },
   "outputs": [],
   "source": [
    "# model 4\n",
    "model_4 = gensim.models.Word2Vec (\n",
    "    vector_size=100,   \n",
    "    window=2,   \n",
    "    min_count=2, \n",
    "    workers=10,  \n",
    "    sg=1,        \n",
    "    hs=0,\n",
    "    negative=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zaKTwj45IQHf",
   "metadata": {
    "id": "zaKTwj45IQHf"
   },
   "source": [
    "**Parameters' selection criteria**\n",
    "\n",
    "Two pairs of hyperparameters' sets were formulated in order to further spot any significant difference between the two architectures (Skip-gram and CBOW). \n",
    "\n",
    "Pair 1: Model 1 (CBOW) & Model 2 (Skip-gram)\n",
    "\n",
    "Pair 2: Model 3 (CBOW) & Model 4 (Skip-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sXMWlJ_BZlaP",
   "metadata": {
    "id": "sXMWlJ_BZlaP"
   },
   "source": [
    "**3. Train models & save to disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba16a1ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba16a1ce",
    "outputId": "aa78e418-5710-40ad-9bc3-f02ab1296df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1 is training...\n",
      "model_1 trained and saved succesfully\n",
      "model_2 is training...\n",
      "model_2 trained and saved succesfully\n",
      "model_3 is training...\n",
      "model_3 trained and saved succesfully\n",
      "model_4 is training...\n",
      "model_4 trained and saved succesfully\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for model in [model_1, model_2, model_3, model_4]:\n",
    "  \n",
    "  model.build_vocab (sentences, progress_per = 1000)\n",
    "  \n",
    "  print (f'model_{i} is training...')\n",
    "\n",
    "  model.train(sentences, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "  model.save (f\"models/model_{i}.model\")\n",
    "\n",
    "  print (f'model_{i} trained and saved succesfully')\n",
    "\n",
    "  i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RaoFK1OS9lH-",
   "metadata": {
    "id": "RaoFK1OS9lH-"
   },
   "source": [
    "**4. Models' Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fq0BTMApFfQY",
   "metadata": {
    "id": "fq0BTMApFfQY"
   },
   "source": [
    "**Load models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "UsfGHqwG_wHj",
   "metadata": {
    "id": "UsfGHqwG_wHj"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_1 = gensim.models.Word2Vec.load(\"models/model_1.model\")\n",
    "model_2 = gensim.models.Word2Vec.load(\"models/model_2.model\")\n",
    "model_3 = gensim.models.Word2Vec.load(\"models/model_3.model\")\n",
    "model_4 = gensim.models.Word2Vec.load(\"models/model_4.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9ZQ3bGFvzlW",
   "metadata": {
    "id": "j9ZQ3bGFvzlW"
   },
   "source": [
    "**Similar words to a given one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a5f03c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a5f03c4",
    "outputId": "240fce2b-b502-4d11-af4a-28bc76a694ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model_1:\n",
      "\n",
      "words similar to important:\n",
      " [('essential', 0.686482310295105), ('obstacle', 0.6447576880455017), ('successful', 0.6429343819618225), ('crucial', 0.6400361061096191), ('ideal', 0.6318754553794861), ('priority', 0.6281493902206421), ('difficult', 0.6200161576271057), ('factor', 0.6175982356071472), ('goal', 0.6050266027450562), ('importance', 0.6047756671905518)]\n",
      "words similar to police:\n",
      " [('officers', 0.7701618671417236), ('kfor', 0.7123222351074219), ('prosecutors', 0.6956273913383484), ('eulex', 0.6943183541297913), ('soldiers', 0.6774521470069885), ('courts', 0.672831654548645), ('kps', 0.6711850762367249), ('army', 0.6609285473823547), ('peacekeepers', 0.6510785222053528), ('force', 0.6365142464637756)]\n",
      "words similar to good:\n",
      " [('normal', 0.7690985798835754), ('very', 0.7430737614631653), ('great', 0.7209277749061584), ('always', 0.6905218362808228), ('bad', 0.6854774355888367), ('absolutely', 0.6430114507675171), ('hard', 0.634160041809082), ('really', 0.6265292167663574), ('positive', 0.6199281811714172), ('success', 0.6160067319869995)]\n",
      "\n",
      "model_2:\n",
      "\n",
      "words similar to important:\n",
      " [('crucial', 0.6131486892700195), ('sensitive', 0.6106196045875549), ('element', 0.5804253816604614), ('vital', 0.5630052089691162), ('shaping', 0.5597473382949829), ('importantly', 0.5583673119544983), ('desirable', 0.5522439479827881), ('essential', 0.5501478910446167), ('aspect', 0.5479042530059814), ('logical', 0.5421854853630066)]\n",
      "words similar to police:\n",
      " [('officers', 0.7009467482566833), ('kps', 0.5747312307357788), ('policemen', 0.5609288811683655), ('checkpoints', 0.533462643623352), ('sting', 0.516157865524292), ('patrols', 0.5091467499732971), ('escort', 0.5063806176185608), ('searched', 0.5061479210853577), ('brnjak', 0.5047496557235718), ('courthouse', 0.5025792717933655)]\n",
      "words similar to good:\n",
      " [('neighbourly', 0.5999771356582642), ('bad', 0.5817784667015076), ('excellent', 0.5720838904380798), ('useful', 0.5587018132209778), ('rewarded', 0.553356409072876), ('inevitable', 0.5530704259872437), ('neighbors', 0.5478588342666626), ('tactical', 0.5440557599067688), ('understood', 0.5379880666732788), ('essence', 0.5367523431777954)]\n",
      "\n",
      "model_3:\n",
      "\n",
      "words similar to important:\n",
      " [('difficult', 0.8110690116882324), ('essential', 0.8055753111839294), ('sensitive', 0.7754358053207397), ('critical', 0.7675163149833679), ('crucial', 0.76352459192276), ('successful', 0.7402272820472717), ('vital', 0.7144882082939148), ('considered', 0.6931527853012085), ('example', 0.6905236840248108), ('dangerous', 0.6831673979759216)]\n",
      "words similar to police:\n",
      " [('courts', 0.7184013724327087), ('prosecutors', 0.7144643068313599), ('kfor', 0.7023389935493469), ('peacekeepers', 0.6618180274963379), ('troops', 0.6475866436958313), ('officers', 0.6438731551170349), ('soldiers', 0.6403133869171143), ('judges', 0.6343370079994202), ('authorities', 0.6321717500686646), ('eulex', 0.6310543417930603)]\n",
      "words similar to good:\n",
      " [('bad', 0.7600467801094055), ('normal', 0.7405083775520325), ('constructive', 0.7203970551490784), ('great', 0.7134239673614502), ('tough', 0.7100428342819214), ('positive', 0.694860577583313), ('better', 0.6944857239723206), ('crucial', 0.684514582157135), ('big', 0.6828758716583252), ('very', 0.6822417974472046)]\n",
      "\n",
      "model_4:\n",
      "\n",
      "words similar to important:\n",
      " [('crucial', 0.795605480670929), ('sensitive', 0.7925959825515747), ('essential', 0.7720969915390015), ('challenging', 0.7659703493118286), ('difficult', 0.7483887672424316), ('vital', 0.7482602596282959), ('interesting', 0.7472650408744812), ('valuable', 0.74390709400177), ('logical', 0.7390926480293274), ('critical', 0.7340950965881348)]\n",
      "words similar to police:\n",
      " [('kps', 0.6784286499023438), ('officers', 0.674872100353241), ('kfor', 0.6695204377174377), ('peacekeepers', 0.6692132353782654), ('prosecutors', 0.6559366583824158), ('eulex', 0.6473762392997742), ('authorities', 0.6391156911849976), ('courts', 0.6352749466896057), ('armed', 0.6274497509002686), ('sfor', 0.6233097314834595)]\n",
      "words similar to good:\n",
      " [('bad', 0.7677273750305176), ('normal', 0.7652623057365417), ('great', 0.7056931853294373), ('neighbourly', 0.6942143440246582), ('better', 0.6823379993438721), ('useful', 0.6804978847503662), ('realistic', 0.6772060990333557), ('delicate', 0.6749579906463623), ('positive', 0.6738989949226379), ('friendly', 0.6733461022377014)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 1\n",
    "\n",
    "for model in [model_1, model_2, model_3, model_4]:\n",
    "  \n",
    "  print (f'\\nmodel_{i}:\\n')\n",
    "\n",
    "  words = ['important', 'police', 'good']\n",
    "  for word in words:\n",
    "    \n",
    "    print (f'words similar to {word}:\\n', model.wv.most_similar(word))\n",
    "  i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7GyikDq4wHk-",
   "metadata": {
    "id": "7GyikDq4wHk-"
   },
   "source": [
    "**Similarity between pairs of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "wy4vqDHHproW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wy4vqDHHproW",
    "outputId": "29788c64-63ba-4fb4-f705-fb795dcbb6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model_1:\n",
      "\n",
      "\n",
      " similarity between words \"girl\" and \"boy\":\t85.75131893157959%\n",
      "\n",
      " similarity between words \"country\" and \"rock\":\t-13.031578063964844%\n",
      "\n",
      "model_2:\n",
      "\n",
      "\n",
      " similarity between words \"girl\" and \"boy\":\t88.66469264030457%\n",
      "\n",
      " similarity between words \"country\" and \"rock\":\t13.84054720401764%\n",
      "\n",
      "model_3:\n",
      "\n",
      "\n",
      " similarity between words \"girl\" and \"boy\":\t92.05288290977478%\n",
      "\n",
      " similarity between words \"country\" and \"rock\":\t0.5180047824978828%\n",
      "\n",
      "model_4:\n",
      "\n",
      "\n",
      " similarity between words \"girl\" and \"boy\":\t89.61658477783203%\n",
      "\n",
      " similarity between words \"country\" and \"rock\":\t16.82732403278351%\n"
     ]
    }
   ],
   "source": [
    " i = 1\n",
    "\n",
    "for model in [model_1, model_2, model_3, model_4]:\n",
    "  print (f'\\nmodel_{i}:\\n')\n",
    "  similarity = model.wv.similarity (w1='girl',w2='boy')\n",
    "  diversity = model.wv.similarity (w1='country',w2='rock')  \n",
    "  print (f'\\n similarity between words \"girl\" and \"boy\":\\t{similarity*100}%')\n",
    "  print (f'\\n similarity between words \"country\" and \"rock\":\\t{diversity*100}%')\n",
    "\n",
    "  i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5HsXs7C4E9it",
   "metadata": {
    "id": "5HsXs7C4E9it"
   },
   "source": [
    "**Words that do not match**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "OJ2h-GlRDfrH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJ2h-GlRDfrH",
    "outputId": "685f3836-a32e-4201-c39b-8ff00033330b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the irrelevant word in group [breakfast paris lunch dinner] is paris (model_1)\n",
      "the irrelevant word in group [breakfast paris lunch dinner] is paris (model_2)\n",
      "the irrelevant word in group [breakfast paris lunch dinner] is paris (model_3)\n",
      "the irrelevant word in group [breakfast paris lunch dinner] is paris (model_4)\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for model in [model_1, model_2, model_3, model_4]:\n",
    "  word = model.wv.doesnt_match(\"breakfast paris lunch dinner\".split())\n",
    "  print (f'the irrelevant word in group [breakfast paris lunch dinner] is {word} (model_{i})')\n",
    "\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GTk2cis73ZY6",
   "metadata": {
    "id": "GTk2cis73ZY6"
   },
   "source": [
    "**Word analogies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "WBxSikk41c3W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBxSikk41c3W",
    "outputId": "eff41412-3360-460d-84d7-99c132837ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analogy paris:france;berlin:germany is 94.58596706390381% true (model_1)\n",
      "analogy paris:france;berlin:germany is 80.85355758666992% true (model_2)\n",
      "analogy paris:france;berlin:germany is 96.04023694992065% true (model_3)\n",
      "analogy paris:france;berlin:germany is 89.81183767318726% true (model_4)\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for model in [model_1, model_2, model_3, model_4]:\n",
    "  \n",
    "  similarity = model.wv.n_similarity(['paris', 'france'], ['berlin', 'germany'])\n",
    "  \n",
    "  print(f\"analogy paris:france;berlin:germany is {similarity*100}% true (model_{i})\")\n",
    "\n",
    "  i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gNOx-cMALSqz",
   "metadata": {
    "id": "gNOx-cMALSqz"
   },
   "source": [
    "**Comments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAguiUV4Lrh3",
   "metadata": {
    "id": "FAguiUV4Lrh3"
   },
   "source": [
    "Based on the selected evaluation tests we can comment the following:\n",
    "\n",
    "**1. Smaller window size and word sub-sampling threshold (word count) lead to better performances in all tests (Pair 2, window size: 2, word count: 2 > Pair 1, window size: 10, word count: 10).**\n",
    "\n",
    "Although **large window size** slection means less syntactic constricted information due to larger margins of context words that are included in the training, it is higly possible that a larger number of words that are **semanticly irrelevant to the given word are taken into account by the model** (e.g. model 2 gave the word 'neighbourly' as similar to \"good\"). Moreover, a **high word frequency limitation**, namely the more frequent a word is the less possible is to be included in the models' training vocabulary,  **leads to the exclusion of less frequent words** in a dataset but more semanticly related to the given word (e.g. model 1 gave the word \"very\" as more similar to the word \"good\" than the words \"great\" or \"bad\" which was not the case with model 3)\n",
    "\n",
    "**2. CBOW architecture performed better with more frequent words than skip-gram but worse with rarer words or word connections.**\n",
    "\n",
    "In the first test, CBOW models' similarity predictions with given word  \"important\" were slightly worse than the ones of Skip-grams models (e.g. cbows: essential, obstacle, difficult, sensitive / skip-grams: crucial, sensitive, vital, essential,) in contrast with the more frequent word \"good\" where the predictions made by CBOW models were better (e.g. cbows: bad, normal, very, great / skip-grams: neighbourly, bad, excellent, normal). In the fourth test (word pair analogy), where the test pair of vector differences is related to countries and capitals, CBOW model outperformed Skip-gram models due to the high frequency of these words in the news domain specific training dataset.\n",
    "\n",
    "The above observations are explained by the nature of the two different architectures. **CBOW is learning to predict the word by the context** therefore a rare word will get much less attention of the model because it is designed to predict the most probable word. On the other hand, **the skip-gram model is designed to predict the context** which gives more attention to the given word despite the rarity. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
